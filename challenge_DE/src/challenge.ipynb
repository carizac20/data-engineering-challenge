{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RRESPUESTAS (accerder al link) https://console.cloud.google.com/storage/browser/farmerstweets;tab=objects?forceOnBucketsSortingFiltering=true&authuser=1&project=etl-latam&prefix=&forceOnObjectsSortingFiltering=false\n",
    "\n",
    "\n",
    "LINK JSON = https://drive.google.com/file/d/1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis/view\n",
    "\n",
    "En este proyecto, me propuse crear un proceso de ETL (Extracción, Transformación y Carga) para analizar un conjunto de tweets, enfrentándome a un reto significativo debido a las restricciones de seguridad en Google Cloud Platform (GCP) que impedían el acceso directo a la API para archivos de gran tamaño. La solución adoptada fue la descarga manual de un archivo JSON desde un enlace específico en GCP, lo que permitió eludir los requerimientos de autenticación API de Google, facilitando así el proceso de extracción.\n",
    "\n",
    "El inicio de este proceso involucró una exhaustiva fase de exploración de datos utilizando el script test_data.py. Este análisis preliminar fue vital para entender los datos en detalle, incluyendo la identificación de registros nulos y duplicados, y establecer un contexto apropiado para las consultas analíticas subsiguientes. Utilicé PySpark para diseñar un esquema que estructurara claramente los datos, determinando así los campos necesarios para abordar las preguntas específicas del análisis.\n",
    "\n",
    "Para manipular y transformar los datos de manera efectiva, recurrí a una serie de bibliotecas de Python especializadas, entre ellas Pandas para la manipulación de datos, PySpark para el procesamiento de datos a gran escala, Collections para el manejo avanzado de estructuras de datos, y JSON para la serialización y deserialización de los datos.\n",
    "\n",
    "Aunque la carga del archivo JSON requería una intervención manual para su ejecución, el proyecto también contempló el uso de un bucket en Google Cloud Platform (GCP) con el objetivo de aprovechar los servicios en la nube durante la fase de carga del proceso ETL. Este enfoque implicó la configuración cuidadosa del bucket para satisfacer los requisitos del proyecto y garantizar su accesibilidad. Sin embargo, es importante destacar que la dependencia principal del proyecto no era el bucket, sino la capacidad de manejar manualmente el archivo JSON para ejecutar el código.\n",
    "\n",
    "A lo largo del proyecto, se hizo evidente que diferentes enfoques de optimización, ya sea en términos de tiempo de ejecución o eficiencia de memoria, pueden resultar en variaciones significativas en el rendimiento. Este análisis destacó la importancia de evaluar distintas estrategias de optimización para mejorar el código, observando cómo algunas metodologías pueden reducir los picos de uso tanto de tiempo como de memoria, ofreciendo así vías para la mejora continua del proceso ETL.\n",
    "\n",
    "En conclusión, este proyecto no solo abordó los desafíos técnicos de trabajar con datos a gran escala en un entorno de nube, sino que también proporcionó insights valiosos sobre la optimización de código, subrayando la importancia de una planificación y ejecución cuidadosas para maximizar la eficiencia y efectividad del proceso de ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\CAMILO\\OneDrive\\Documents\\GitHub\\data-engineering-challenge\\tweets\\farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\CAMILO\\\\OneDrive\\\\Documents\\\\GitHub\\\\data-engineering-challenge\\\\tweets\\\\farmers-protest-tweets-2021-2-4.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mq1_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m q1_memory \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbucket\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe_to_gcs_json\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemory_profiler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CAMILO\\OneDrive\\Documents\\GitHub\\data-engineering-challenge\\challenge_DE\\src\\q1_memory.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCAMILO\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata-engineering-challenge\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfarmers-protest-tweets-2021-2-4.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Llamar a la función q1_memory para obtener los resultados\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m top_results \u001b[38;5;241m=\u001b[39m \u001b[43mq1_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#Imprimir los resultados sin el contador\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#print(\"Top 10 fechas con más tweets y el usuario con más publicaciones:\")\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#for i, (date, username) in enumerate(top_results, start=1):\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#    print(f\"{i}. Fecha: {date}, Usuario con más publicaciones: {username}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CAMILO\\OneDrive\\Documents\\GitHub\\data-engineering-challenge\\challenge_DE\\src\\q1_memory.py:8\u001b[0m, in \u001b[0;36mq1_memory\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mq1_memory\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m      6\u001b[0m     user_counts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m     10\u001b[0m             tweet \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\CAMILO\\\\OneDrive\\\\Documents\\\\GitHub\\\\data-engineering-challenge\\\\tweets\\\\farmers-protest-tweets-2021-2-4.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from q1_memory import q1_memory \n",
    "from bucket import dataframe_to_gcs_json\n",
    "%load_ext memory_profiler \n",
    "\n",
    "\n",
    "# Ejecuta la función y obtén los resultados\n",
    "resultados = q1_memory(file_path)\n",
    "%time results = q1_memory(file_path)\n",
    "%memit q1_memory(file_path)\n",
    "\n",
    "# Convertir los resultados en un DataFrame de pandas\n",
    "df = pd.DataFrame(resultados, columns=['fecha', 'usuario'])\n",
    "\n",
    "# Muestra el DataFrame\n",
    "# df\n",
    "dataframe_to_gcs_json(df, \"farmerstweets\", \"q1_memory.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_file.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mq1_time\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m q1_time\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ejecuta la función y obtén los resultados\u001b[39;00m\n\u001b[0;32m      6\u001b[0m resultados \u001b[38;5;241m=\u001b[39m q1_time(file_path)\n",
      "File \u001b[1;32mc:\\Users\\CAMILO\\OneDrive\\Documents\\GitHub\\data-engineering-challenge\\challenge_DE\\src\\q1_time.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Suponiendo que tienes el archivo JSON en la ruta 'file_path':\u001b[39;00m\n\u001b[0;32m     30\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_your_file.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 31\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mq1_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\CAMILO\\OneDrive\\Documents\\GitHub\\data-engineering-challenge\\challenge_DE\\src\\q1_time.py:11\u001b[0m, in \u001b[0;36mq1_time\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m date_user_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Abre el archivo y procesa cada línea individualmente.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m     13\u001b[0m         tweet \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_file.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from q1_time import q1_time\n",
    "\n",
    "\n",
    "# Ejecuta la función y obtén los resultados\n",
    "resultados = q1_time(file_path)\n",
    "%time results = q1_time(file_path)\n",
    "%memit q1_time(file_path)\n",
    "\n",
    "# Convertir los resultados en un DataFrame de pandas\n",
    "df = pd.DataFrame(resultados, columns=['fecha', 'usuario'])\n",
    "\n",
    "# Muestra el DataFrame\n",
    "# df\n",
    "dataframe_to_gcs_json(df, \"farmerstweets\", \"q1_time.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Los top 10 emojis más usados con su respectivo conteo. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "CPU times: total: 20.8 s\n",
      "Wall time: 21.5 s\n",
      "peak memory: 135.97 MiB, increment: 0.02 MiB\n",
      "creds <google.oauth2.service_account.Credentials object at 0x0000020490CA9C40>\n",
      "DataFrame uploaded to q2_memory.json in bucket farmerstweets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from q2_memory import q2_memory\n",
    "%load_ext memory_profiler\n",
    "%time results = q2_memory(file_path)\n",
    "\n",
    "# Ejecuta la función y obtén los resultados\n",
    "resultados = q2_memory(file_path)\n",
    "%memit q2_memory(file_path)\n",
    "\n",
    "# Convierte los resultados en un DataFrame de pandas\n",
    "df = pd.DataFrame(resultados, columns=['emoji', 'count'])\n",
    "\n",
    "# Muestra el DataFrame\n",
    "# df\n",
    "dataframe_to_gcs_json(df, \"farmerstweets\", \"q2_memory.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "CPU times: total: 10.2 s\n",
      "Wall time: 10.6 s\n",
      "peak memory: 137.03 MiB, increment: 0.01 MiB\n",
      "creds <google.oauth2.service_account.Credentials object at 0x000002049F5677D0>\n",
      "DataFrame uploaded to q2_time.json in bucket farmerstweets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from q2_time import q2_time\n",
    "%load_ext memory_profiler\n",
    "%time results = q2_time(file_path)\n",
    "\n",
    "# Ejecuta la función y obtén los resultados\n",
    "resultados = q2_memory(file_path)\n",
    "%memit q2_time(file_path)\n",
    "\n",
    "# Convierte los resultados en un DataFrame de pandas\n",
    "df = pd.DataFrame(resultados, columns=['emoji', 'count'])\n",
    "\n",
    "# Muestra el DataFrame\n",
    "# df\n",
    "dataframe_to_gcs_json(df, \"farmerstweets\", \"q2_time.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "CPU times: total: 8.88 s\n",
      "Wall time: 9.62 s\n",
      "peak memory: 462.10 MiB, increment: 192.48 MiB\n",
      "creds <google.oauth2.service_account.Credentials object at 0x00000204A08F0920>\n",
      "DataFrame uploaded to q3_memory.json in bucket farmerstweets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from q3_memory import q3_memory  # Asumiendo que q3_memory está definida en otro archivo Python\n",
    "%load_ext memory_profiler\n",
    "%time results = q3_memory(file_path)\n",
    "\n",
    "\n",
    "# Ejecuta la función y obtén los resultados\n",
    "resultados = q3_memory(file_path)\n",
    "%memit q3_memory(file_path)\n",
    "\n",
    "# Convierte los resultados en un DataFrame de pandas\n",
    "# Asegúrate de que las columnas correspondan a la salida de tu función q3_memory\n",
    "df = pd.DataFrame(resultados, columns=['username', 'count@'])\n",
    "\n",
    "# Muestra el DataFrame\n",
    "# df\n",
    "dataframe_to_gcs_json(df, \"farmerstweets\", \"q3_memory.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "CPU times: total: 5.19 s\n",
      "Wall time: 5.77 s\n",
      "peak memory: 290.45 MiB, increment: 0.19 MiB\n",
      "creds <google.oauth2.service_account.Credentials object at 0x00000204A1A55310>\n",
      "DataFrame uploaded to q3_time.json in bucket farmerstweets.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from q3_time import q3_time  # Asumiendo que q3_memory está definida en otro archivo Python\n",
    "%load_ext memory_profiler\n",
    "%time results = q3_time(file_path)\n",
    "\n",
    "# Ejecuta la función y obtén los resultados\n",
    "resultados = q3_memory(file_path)\n",
    "%memit q3_time(file_path)\n",
    "\n",
    "# Convierte los resultados en un DataFrame de pandas\n",
    "# Asegúrate de que las columnas correspondan a la salida de tu función q3_memory\n",
    "df = pd.DataFrame(resultados, columns=['username', 'count@'])\n",
    "\n",
    "# Muestra el DataFrame\n",
    "# df\n",
    "dataframe_to_gcs_json(df, \"farmerstweets\", \"q3_time.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
